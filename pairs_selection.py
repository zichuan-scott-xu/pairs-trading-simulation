# -*- coding: utf-8 -*-
"""pairs-trading-wenqi+scott.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ksa_tM5w-yf8w0cohkHopgG_EbqTG04X
"""

# Commented out IPython magic to ensure Python compatibility.
import math
import yfinance as yf
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, TensorDataset
import tqdm
import statsmodels.api as sm
from datetime import datetime
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.decomposition import PCA
import matplotlib.gridspec as gridspec
from sklearn.cluster import OPTICS
# %matplotlib inline

# Hand pick some stocks

tech = ['AAPL', 'AMZN', 'GOOGL', 'INTC', 'NVDA', 'NFLX', 'MSFT', 'META', 'ORCL', 'UBER']
food_drink = ['MCD', 'SBUX', 'PEP', 'KO', 'NSRGF', 'KHC', 'BUD', 'HSY', 'GIS']
biotech = ['PFE', 'MRK', 'AMGN', 'JNJ', 'GILD', 'BMY', 'AZN', 'LLY', 'ABBV']
energy = ['XOM', 'CVX', 'BP', 'TTE', 'SHEL', 'COP', 'SLB', 'PSX', 'HAL', 'E']
real_estate = ['SPG', 'PLD', 'AMT', 'PSA', 'EQIX', 'WELL', 'AVB', 'O', 'DLR', 'VTR']

list_of_stocks = real_estate + tech + food_drink + biotech + energy

stocks500df = pd.read_csv('SP500.csv')
list_of_stocks = stocks500df['Symbol']

sd = datetime(2020, 5, 6) 
ed = datetime(2023, 5, 5)
stocks = {}
stocks_array = np.zeros((len(list_of_stocks), 755))

for i in range(len(list_of_stocks)):
  label = list_of_stocks[i]
  stocks[label] = yf.download(tickers=label, start=sd, end=ed, interval="1d")
  stocks_array[i] = stocks[label]['Adj Close'].to_numpy().flatten()

print(stocks['AAPL'])
print(stocks['AMZN'].to_numpy().shape)

normalized_stocks_array = np.zeros(stocks_array.shape)
for i in range(len(list_of_stocks)):
  normalized_stocks_array[i, 0] = 0
  for j in range(1, 755):
    normalized_stocks_array[i, j] = (stocks_array[i, j] - stocks_array[i, j-1]) / stocks_array[i, j-1]

pca = PCA(n_components=15)
reduced_stock_array = pca.fit_transform(normalized_stocks_array)

print(reduced_stock_array.shape)

optics_clustering = OPTICS(min_samples=3).fit(reduced_stock_array)
print(optics_clustering.labels_)



plt.figure(figsize=(7, 7))
G = gridspec.GridSpec(1, 1)
ax2 = plt.subplot(G[0, 0])
clusters = []

colors = ["g.", "r.", "b.", "y.", "c."]
for klass, color in zip(range(0, 5), colors):
    Xk = reduced_stock_array[optics_clustering.labels_ == klass]
    ax2.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.4)
    indices = [i for i, x in enumerate(optics_clustering.labels_) if x == klass]
    cur_list = [list_of_stocks[i] for i in indices]
    clusters.append(cur_list)
    print(f'Cluster {klass} contains {cur_list}')
ax2.plot(reduced_stock_array[optics_clustering.labels_ == -1, 0], reduced_stock_array[optics_clustering.labels_ == -1, 1], "k+", alpha=0.1)
ax2.set_title("Automatic Clustering\nOPTICS")

def get_common_dates(stock1, stock2):
    return list(set(stock1.index.tolist()).intersection(set(stock2.index.tolist())))

def get_hurst_exponent(time_series, max_lag=20):
    """Returns the Hurst Exponent of the time series"""
    
    lags = range(2, max_lag)

    # variances of the lagged differences
    tau = [np.std(np.subtract(time_series[lag:], time_series[:-lag])) for lag in lags]

    # calculate the slope of the log plot -> the Hurst Exponent
    reg = np.polyfit(np.log(lags), np.log(tau), 1)

    return reg[0]

def reg(stock1_common, stock2_common):
    price1, price2 = np.array(stock1_common['Adj Close']), np.array(stock2_common['Adj Close'])
    logprice1, logprice2 = np.log(price1), np.log(price2)
    lin_reg = LinearRegression(fit_intercept=True)
    lin_reg.fit(logprice1.reshape(-1, 1), logprice2)
    beta, alpha = lin_reg.coef_[0], lin_reg.intercept_
    spread = logprice2 - beta * logprice1 - alpha
    
    return spread

def ADFTest(spread):
    adf = sm.tsa.stattools.adfuller(spread, maxlag=1)
    print('ADF test statistic: %.02f' % adf[0])
    for key, value in adf[4].items():
        print('\t%s: %.3f' % (key, value))
    print('p-value: %.03f' % adf[1])
    return adf[1]

def half_life(spread):
    z_lag = np.roll(spread,1)
    z_lag[0] = 0
    z_ret = spread - z_lag
    z_ret[0] = 0
   
    # adds intercept terms to X variable for regression
    z_lag2 = sm.add_constant(z_lag)

    # model = sm.OLS(z_ret,z_lag2)
    # res = model.fit()

    z_lag2 = sm.add_constant(z_lag)

    model = sm.OLS(z_ret,z_lag2)
    res = model.fit()

    # lin_reg = LinearRegression(fit_intercept=True)
    # lin_reg.fit(z_lag.reshape(-1, 1), z_ret)

    # halflife = np.log(0.5) / lin_reg.coef_[0]
    halflife = -np.log(2) / res.params[1]
    # print(res.params[1])

    return halflife

def num_of_hitting_zero(spread):
  count = 0
  for i in range(spread.shape[0] - 1):
    if spread[i] * spread[i+1] < 0:
      count += 1
  print(f'Hit zero {count} times')
  return count



def testPair(stock1_name, stock2_name, sd, ed):
    ENDC = '\033[0m'
    PASS = '\033[92m'
    FAIL = '\033[91m'
    stock1 = yf.download(tickers=stock1_name, start=sd, end=ed, interval="1d")
    stock2 = yf.download(tickers=stock2_name, start=sd, end=ed, interval="1d")  
    common_dates = sorted(get_common_dates(stock1, stock2))
    stock1_common, stock2_common = stock1.loc[common_dates], stock2.loc[common_dates]
    spread = reg(stock1_common, stock2_common)
    
    ADF_p_value = ADFTest(spread)
    # plt.plot(common_dates, spread)
    if ADF_p_value > 0.05:
        print("\033[91mNot a Pair!" + ENDC)
        return False, (stock1_name, stock2_name)
    hurst_exponent = get_hurst_exponent(spread)
    print(f'Hurst exponent is {hurst_exponent}')
    if hurst_exponent > 0.5:
        print(FAIL + "Not a Pair!" + ENDC)
        return False, (stock1_name, stock2_name)
    print(f'Half life of mean reversion is {half_life(spread)}')
    if half_life(spread) < 1 or half_life(spread) > 365:
        print(FAIL + "Not a Pair!" + ENDC)
        return False, (stock1_name, stock2_name)
    if num_of_hitting_zero(spread) < 12*3:
        print(FAIL + "Not a Pair!" + ENDC)
        return False, (stock1_name, stock2_name)
    print(PASS + stock1_name + " and " + stock2_name + " are indeed a pair!" + ENDC)
    return True, (stock1_name, stock2_name)

# testPair('KO', 'HSY', sd, ed)

counter = 0
pair_counter = 0
pairs = []
for cluster in clusters:
  for i in range(len(cluster)):
    for j in range(len(cluster)):
      if i < j:
        counter += 1
        areTheyPair, pair = testPair(cluster[i], cluster[j], sd, ed)
        if areTheyPair:
          pair_counter += 1
          pairs.append(pair)
print(f'{counter} potential pairs are tested, {pair_counter} actual pairs are found')
for pair in pairs:
  print(pair)

